# -*- coding: utf-8 -*-
"""LLaMA-LoRA-PubMed-FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S_HpCLKKtXOkttPZh0oD0pjQskMDN-Fz
"""

import torch

# Check if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")



!pip install opencv-python

! pip install transformers peft datasets accelerate

"""### Fine-Tune the Model with LoRA"""

! huggingface-cli login

# Step 1: Install Required Libraries
# Run this in your terminal or notebook
# !pip install transformers peft datasets accelerate ftplib gzip shutil

# Step 2: Download 10% of PubMed Files
import ftplib
import os
import random

# --- Configuration ---
SUBSET_RATIO = 0.01  # 2% of total files (adjust between 0.01-1.0)
RANDOM_SEED = 42    # For reproducibility
# ---------------------

# FTP server details
ftp_server = "ftp.ncbi.nlm.nih.gov"
ftp_directory = "/pubmed/baseline/"
local_directory = "./pubmed_subset"

# Create local directory if it doesn't exist
os.makedirs(local_directory, exist_ok=True)

# Connect to the FTP server
ftp = ftplib.FTP(ftp_server)
ftp.login()  # Anonymous login

# Change to the PubMed directory
ftp.cwd(ftp_directory)

# Get list of XML files only
all_files = [f for f in ftp.nlst() if f.endswith(".xml.gz")]
print(f"Total files available: {len(all_files)}")

# Select random subset
random.seed(RANDOM_SEED)
subset_size = int(len(all_files) * SUBSET_RATIO)
selected_files = random.sample(all_files, subset_size)
print(f"Downloading {len(selected_files)} files ({SUBSET_RATIO*100}% subset)")

# Download only selected files
for file in selected_files:
    local_path = os.path.join(local_directory, file)
    with open(local_path, "wb") as f:
        ftp.retrbinary(f"RETR {file}", f.write)
    print(f"Downloaded {file}")

# Close the FTP connection
ftp.quit()

# Step 3: Extract Downloaded Files
import gzip
import shutil

print("Extracting subset files...")
for file in os.listdir(local_directory):
    if file.endswith(".gz"):
        gz_path = os.path.join(local_directory, file)
        xml_path = os.path.join(local_directory, file[:-3])  # Remove .gz extension

        with gzip.open(gz_path, "rb") as f_in:
            with open(xml_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)
        os.remove(gz_path)  # Delete .gz after extraction

print(f"Extracted {len(os.listdir(local_directory))} XML files")

# Step 4: Load and Process Data
from datasets import Dataset
import xml.etree.ElementTree as ET

# Parse XML files and extract abstracts
def safe_parse(file_path):
    try:
        tree = ET.parse(file_path)
        return [
            {"article": article.findtext(".//AbstractText") or ""}
            for article in tree.findall(".//PubmedArticle")
        ]
    except ET.ParseError:
        return []

# Process only the subset files
articles = []
for file in os.listdir(local_directory):
    if file.endswith(".xml"):
        articles.extend(safe_parse(os.path.join(local_directory, file)))

# Create dataset from subset
dataset = Dataset.from_dict({"article": [a["article"] for a in articles if a["article"]]})

# Split into train/validation sets
dataset = dataset.train_test_split(test_size=0.1, seed=RANDOM_SEED)  # 90% train, 10% validation
print(f"Train size: {len(dataset['train'])}, Validation size: {len(dataset['test'])}")

# Step 5: Tokenization (FIXED VERSION)
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")

# Configure padding explicitly
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

def tokenize_function(examples):
    # Tokenize inputs and create labels
    tokenized = tokenizer(
        examples["article"],
        padding="max_length",
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    # Add labels for causal language modeling
    tokenized["labels"] = tokenized["input_ids"].clone()
    return tokenized

# Tokenize the dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["article"])

# If tokenized_dataset is a DatasetDict, extract the 'train' split
if isinstance(tokenized_dataset, dict):
    tokenized_dataset = tokenized_dataset["train"]  # Extract the 'train' split

# Split into train/validation sets
split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)  # 90% train, 10% validation
tokenized_train = split_dataset["train"]
tokenized_val = split_dataset["test"]

# Step 6: Fine-Tune the Model with LoRA (FIXED VERSION)
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments

# Load the pre-trained model and move to GPU
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B")
model.to(device)  # Move model to GPU

# Add padding token to model config
model.config.pad_token_id = tokenizer.pad_token_id

# Configure LoRA with fewer parameters
lora_config = LoraConfig(
    r=2,  # Reduced rank (default: 8)
    lora_alpha=4,  # Reduced scaling factor (default: 32)
    target_modules=["q_proj"],  # Target layers to apply LoRA
    lora_dropout=0.05,  # Reduced dropout (default: 0.1)
    bias="none",  # Whether to add bias
    task_type="CAUSAL_LM"  # Task type (causal language modeling)
)

model = get_peft_model(model, lora_config)

# Print the number of trainable parameters
model.print_trainable_parameters()

# Define training arguments for faster training
training_args = TrainingArguments(
    output_dir="./fast-llama",
    per_device_train_batch_size=16,  # Max batch for T4
    num_train_epochs=1,              # Single epoch
    learning_rate=2e-5,
    fp16=True,
    save_strategy="steps",
    save_steps=200,
    logging_steps=10,
    report_to="none",
    max_steps=1000
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,  # Training data
    eval_dataset=tokenized_val,    # Validation data
    tokenizer=tokenizer,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./llama-lora-pubmed-subset")
tokenizer.save_pretrained("./llama-lora-pubmed-subset")

import textwrap

model = AutoModelForCausalLM.from_pretrained("./llama-lora-pubmed-subset")
tokenizer = AutoTokenizer.from_pretrained("./llama-lora-pubmed-subset")
model.to("cuda" if torch.cuda.is_available() else "cpu")

# Generation function with formatting
def generate_formatted_text(prompt, max_length=400):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        inputs.input_ids,
        max_new_tokens=max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Format with wrapping and paragraph breaks
    wrapped_text = textwrap.fill(
        full_text,
        width=80,               # Characters per line
        subsequent_indent='    ', # Indent for wrapped lines
        replace_whitespace=False # Preserve existing newlines
    )
    return wrapped_text

# Generate and print formatted response
prompt = "Recent advances in cancer immunotherapy suggest"
result = generate_formatted_text(prompt)

print("\nGenerated Response:\n" + "="*40)
print(result)
print("="*40)

